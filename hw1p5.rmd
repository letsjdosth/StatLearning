# HW1

Seokjun Choi

## problem 5

*stand-alone R script: https://github.com/letsjdosth/StatLearning/blob/main/hw1p5.r*

### Discussion (in the head!)

The motivation of GCV is to mimic the LOOCV in the ordinary linear regression setting with a shrinkage penalty.
When $S_\lambda$ is a linear smoother of response, $tr(S_\lambda)$ can be viewed as the 'effective' degrees of freedom of the linear smoother,
in the sense that the trace value will be smaller as the smoothing strength parameter $\lambda$ grows.
Note that if $\lambda=0$, $tr(S_0)$ is exactly the same as the model's degrees of freedom in the ordinary sense.

In general, GCV and CV are not exactly the same.
Comparing the properties of GCV with LOOCV, GCV is more robust. 
By using the trace value instead, GCV is less sensitive to each diagonal value of $S_\lambda$, which is badly behaving at influence points. 
Plus, GCV has a good invariance property for orthogonal transformations of data (for example, rotating), while LOOCV does not.
(In detail, see Wakefield, J. (2013). Bayesian and frequentist regression methods, chapter 10)
These features justify using GCV rather than LOOCV when our model's smoother is a linear operator.

Interestingly, the asymptotic properties of GCV and LOOCV in a linear regression setting are the same. (For example, see Shao, J. (1997).)
So with large samples, we can expect that methods depending on GCV and LOOCV will give similar results.
(Note that k-fold CV is different.)

Obviously, GCV calculation is much simpler than LOOCV.
Even if LOOCV has a nice formula in linear smoother cases, it needs the exact 'hat' matrix (smoothing matrix).
But for some models above, getting the matrix requires quite annoying computing, but getting the trace value of the matrix is still relatively easy.



Let's start!
To begin with, let me load packages and data.

```{r, message=FALSE, warning=FALSE}
library(SemiPar)
library(splines)
library(gam)
library(mgcv)

data(fossil)
head(fossil)
plot(strontium.ratio ~ age, data=fossil)
```

We can see a non-linear relationship.

### 5-a. Polinomial regression fit

Let's try four polynomial orders.
In the plot, the black line is the first-order fit. Additionally, blue, red, and green curves are 2nd, 3rd, and 4th order fit, respectively.

```{r}
#polynomial regression
lm1_fit = lm(strontium.ratio ~ age, data=fossil)
lm2_fit = lm(strontium.ratio ~ age+ I(age^2), data=fossil)
lm3_fit = lm(strontium.ratio ~ age + I(age^2) + I(age^3), data=fossil)
lm4_fit = lm(strontium.ratio ~ age + I(age^2) + I(age^3) + I(age^4), data=fossil)
```

```{r, echo=FALSE}
plot(strontium.ratio ~ age, data=fossil)
curve(coef(lm1_fit)[1]+coef(lm1_fit)[2]*x, col="black", add=TRUE)
curve(coef(lm2_fit)[1]+coef(lm2_fit)[2]*x+coef(lm2_fit)[3]*x^2, col="blue", add=TRUE)
curve(coef(lm3_fit)[1]+coef(lm3_fit)[2]*x+coef(lm3_fit)[3]*x^2+coef(lm3_fit)[4]*x^3, col="red", add=TRUE)
curve(coef(lm4_fit)[1]+coef(lm4_fit)[2]*x+coef(lm4_fit)[3]*x^2+coef(lm4_fit)[4]*x^3+coef(lm4_fit)[5]*x^4, col="green", add=TRUE)
```

The first-order and the second-order fits seem too crude.
The third-order and the fourth-order fits are good.
Generally, the simpler model is preferred if two models' performances are comparable.
Thus, I will choose the order-3 polynomial.


```{r, echo=FALSE}
#order 3
pred_df_age = data.frame(age=90:125)
lm3_fit_pred = predict(lm3_fit, pred_df_age, interval="prediction")
plot(strontium.ratio ~ age, data=fossil)
lines(lm3_fit_pred[,1]~pred_df_age$age, col="blue")
lines(lm3_fit_pred[,2]~pred_df_age$age)
lines(lm3_fit_pred[,3]~pred_df_age$age)
```
```{r}
pred_df_age2 = data.frame(age=c(95, 115))
lm3_fit_pred2 = predict(lm3_fit, pred_df_age2, interval="prediction")
print(lm3_fit_pred2)
```

The above plot shows the fitted curve and its 95% interval.
Last part's dataframe output is for prediction interval at $x=95$, and $x=115$.

### 5-b. Cubic spline fit

To choose the number of knots $K$, let me use LOOCV and GCV.

```{r}
# using loocv & GCV
candid_knot_num = 2:20
loocv_for_candid_knot_num = rep(0, length(candid_knot_num))
gcv_for_candid_knot_num = rep(0, length(candid_knot_num))
for(i in 1:length(candid_knot_num)){
    knot_num = candid_knot_num[i]
    knot_position = (125-95)/(knot_num+2)*1:knot_num+95
    lm_ns_fit = lm(strontium.ratio ~ ns(age, knots=knot_position), data=fossil)

    lm_ns_X = model.matrix(lm_ns_fit)
    lm_ns_smoother = lm_ns_X %*% solve(t(lm_ns_X)%*%lm_ns_X) %*% t(lm_ns_X)
    lm_ns_smoother_diag = diag(lm_ns_smoother)
    lm_ns_smoother_trace = sum(lm_ns_smoother_diag)

    lm_ns_loocv = 0
    for(j in 1:length(lm_ns_smoother_diag)){
        lm_ns_loocv = lm_ns_loocv + (lm_ns_fit$residuals[j]/(1 - lm_ns_smoother_diag[j]))^2
    }
    lm_ns_gcv = sum(lm_ns_fit$residuals^2) / (1 - lm_ns_smoother_trace/length(lm_ns_smoother_diag))^2
    loocv_for_candid_knot_num[i] = lm_ns_loocv/length(lm_ns_smoother_diag)
    gcv_for_candid_knot_num[i] = lm_ns_gcv/length(lm_ns_smoother_diag)
}

(candid_knot_num[which.min(loocv_for_candid_knot_num)]) #15
(candid_knot_num[which.min(gcv_for_candid_knot_num)]) #15
```

Among equally-spaced 2, 3, ..., and 20 knots, using 15 knots gave us the least LOOCV and GCV values.
Thus, let's proceed with $K=15$.

```{r}
knot_num = candid_knot_num[which.min(gcv_for_candid_knot_num)]
knot_position = (125-95)/(knot_num+2)*1:knot_num+95
lm_ns_cvknot_fit = lm(strontium.ratio ~ ns(age, knots=knot_position), data=fossil)
# summary(lm_ns_cvknot_fit)
```

```{r, echo=FALSE}
plot(strontium.ratio ~ age, data=fossil)
lm_ns_cvknot_fit_pred = predict(lm_ns_cvknot_fit, pred_df_age, interval="prediction")
lines(lm_ns_cvknot_fit_pred[,1]~pred_df_age$age, col="blue")
lines(lm_ns_cvknot_fit_pred[,2]~pred_df_age$age)
lines(lm_ns_cvknot_fit_pred[,3]~pred_df_age$age)
```

```{r}
lm_ns_cvknot_fit_pred2 = predict(lm_ns_cvknot_fit, pred_df_age2, interval="prediction")
print(lm_ns_cvknot_fit_pred2)
```

The above plot shows the fitted curve and its 95% interval using the 15 knots.
Last part's dataframe output is for prediction interval at $x=95$, and $x=115$.

### 5-c. Smoothing spline fit

First of all, let me use the 'smooth.spline' function in the 'splines' package.

```{r}
## using smooth.spline
candid_df = 5:20
ss_loocv_vec = rep(0, length(candid_df))
ss_gcv_vec = rep(0, length(candid_df))

for(i in 1:length(candid_df)){
    ss_fit_loocv = smooth.spline(fossil$age, fossil$strontium.ratio, nknots=candid_df[i], cv=TRUE)
    ss_loocv_vec[i] = ss_fit_loocv$cv.crit
    ss_fit_gcv = smooth.spline(fossil$age, fossil$strontium.ratio, nknots=candid_df[i], cv=FALSE)
    ss_gcv_vec[i] = ss_fit_gcv$cv.crit
}

(candid_df[which.min(ss_loocv_vec)]) #12
(candid_df[which.min(ss_gcv_vec)]) #12
```

Both LOOCV and GCV choose $nknots=12$.

```{r}
ss_fit_loocv = smooth.spline(fossil$age, fossil$strontium.ratio, nknots=candid_df[which.min(ss_loocv_vec)], cv=TRUE)
ss_fit_gcv = smooth.spline(fossil$age, fossil$strontium.ratio, nknots=candid_df[which.min(ss_gcv_vec)], cv=FALSE)
sum(ss_fit_gcv$lev) #eff number of parameters
ss_fit_loocv_pred = predict(ss_fit_loocv, pred_df_age, interval="prediction", se=TRUE) #no se output
ss_fit_gcv_pred = predict(ss_fit_gcv, pred_df_age, interval="prediction", se=TRUE) #no se output

plot(strontium.ratio ~ age, data=fossil)
lines(ss_fit_loocv_pred$y[,1] ~ ss_fit_loocv_pred$x[,1], col="blue")
lines(ss_fit_gcv_pred$y[,1] ~ ss_fit_gcv_pred$x[,1], col="red")
```

LOOCV fit(blue) and GCV fit(red) are almost identical.
Sadly, the 'smooth.spline' function does not offer a standard error estimate.

To get the SE, I will use the 'mgcv' package.
The 'gam' function automatically chooses the number of splines using GCV.
The argument 'bs="cr"' indicates the smoothing spline fitting.

```{r}
# smoothing spline
ss_gam_fit = gam(strontium.ratio~s(age, bs="cr"), data=fossil)
ss_gam_fit
```

```{r, echo=FALSE}
ss_gam_fit_pred = predict(ss_gam_fit, pred_df_age, interval="prediction", se.fit=TRUE)

plot(strontium.ratio ~ age, data=fossil)
lines(ss_gam_fit_pred$fit~pred_df_age$age, col="blue")
lines(ss_gam_fit_pred$fit + 1.96*ss_gam_fit_pred$se.fit ~ pred_df_age$age)
lines(ss_gam_fit_pred$fit - 1.96*ss_gam_fit_pred$se.fit ~ pred_df_age$age)
```
```{r}
ss_gam_fit_pred2 = predict(ss_gam_fit, pred_df_age2, interval="prediction", se.fit=TRUE)
data.frame(fit=ss_gam_fit_pred2$fit,
            lwr=ss_gam_fit_pred2$fit - 1.96*ss_gam_fit_pred2$se.fit,
            uwr=ss_gam_fit_pred2$fit + 1.96*ss_gam_fit_pred2$se.fit)
```

The above plot shows the fitted curve and its 95% interval.
The 'gam' function gives us a different fitted result from smooth.splines, though.
(Perhaps something is wrong, but I could not find what it is, sadly.)

If applying 1.96 as the 95% prediction intervals' z value relying on asymptotic normality,
the prediction band looks too narrow.
Perhaps there is a better distribution to use, but let me skip exploring it here.

Last part's dataframe output is for prediction interval at $x=95$, and $x=115$.
I think that their interval length is too narrow, as well.

### 5-d. A local regression fit

I will use the 'loess' function in the base-R.
Using GCV, the 'span' argument (smoothing parameter) in the 'loess' function will be selected.

Let me skip using 'LOOCV' here, cause the linear smoother's one term, which is a (tri-cube function's or gaussian's) weight matrix, depends on each $x$ value.
As a result, the smoothing matrix gets a huge dimension, requiring too many calculations to get $S_{ii}$, which is needed to get LOOCV.

I will consider 'span' values 0.1, 0.11, 0.12, ..., 2.

```{r}
#using gcv, loess
candid_span = seq(0.1, 2, 0.01)
gcv_for_candid_span = rep(0, length(candid_span))
for(i in 1:length(candid_span)){
    N = length(fossil$age)
    loess_fit = loess(strontium.ratio ~ age, data=fossil, span=candid_span[i])
    GCV = sum(loess_fit$residuals^2) / (1 -loess_fit$trace.hat/N)^2
    gcv_for_candid_span[i] = GCV
}
(candid_span[which.min(gcv_for_candid_span)])
```

0.24 gave the least GCV. Using the value, let me fit the model.

```{r}
loess_fit = loess(strontium.ratio ~ age, data=fossil, span=candid_span[which.min(gcv_for_candid_span)])
```
```{r, echo=FALSE}
loess_fit_pred = predict(loess_fit, pred_df_age, interval="prediction", se=TRUE)
plot(strontium.ratio ~ age, data=fossil)
lines(loess_fit_pred$fit~pred_df_age$age, col="blue")
lines(loess_fit_pred$fit + qt(0.975, loess_fit_pred$df)*loess_fit_pred$se.fit ~ pred_df_age$age)
lines(loess_fit_pred$fit - qt(0.975, loess_fit_pred$df)*loess_fit_pred$se.fit ~ pred_df_age$age) 
```
```{r}
loess_fit_pred2 = predict(loess_fit, pred_df_age2, interval="prediction", se=TRUE)
data.frame(fit=loess_fit_pred2$fit,
            lwr=loess_fit_pred2$fit - 1.96*loess_fit_pred2$se.fit,
            uwr=loess_fit_pred2$fit + 1.96*loess_fit_pred2$se.fit)
```

Near 95, the interval seems a little wide cause there are only a few data points in the neighborhood.
But, the mean (point) estimation looks good.

The last dataframe output is for prediction interval at $x=95$, and $x=115$.
