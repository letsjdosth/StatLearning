# HW1

Seokjun Choi

## Problem 6

*stand-alone R script: https://github.com/letsjdosth/StatLearning/blob/main/hw1p6.r*

```{r}
# loading data
calif <- read.table("https://raw.githubusercontent.com/jbryer/CompStats/master/Data/cadata.dat",header = TRUE)
calif$MedianOver <- factor(as.numeric(calif$MedianHouseValue > 250000))
cut_calif <- calif[c("MedianOver", "Latitude", "Longitude")]
plot(Latitude ~ Longitude, data=cut_calif, col=cut_calif$MedianOver)

#for fitting check
grid <- data.frame(
    Longitude = rep(seq(min(cut_calif$Longitude), max(cut_calif$Longitude), length.out=100), each=100),
    Latitude = rep(seq(min(cut_calif$Latitude), max(cut_calif$Latitude), length.out=100), times=100)
)
```

Almost all over-median points are clustered at the left-bottom part.
However, we can see that many over- and under-median points overlap in large area.


### 6-a. Support vector machine

To fit SVM, we should pick the 'C' value in the largrangian formula, which tunes cost of constraints violation (soft margin).
I tried values from 1 to 13, and picked 6, which has the best prediction accuracy.

Then, our optimization problem gets to be
\[argmin_{\alpha,\beta,\xi} 6\sum_i \xi_i + \frac{1}{2} ||\beta||^2\]
subject to $Y_i(\alpha + \beta^T X_i) \geq 1-\xi_i$, $i=1,2,...,n$.

```{r, warning=FALSE}
library(e1071)
svm_fit = svm(MedianOver~., data=cut_calif, kernel="linear", scale=FALSE, cost=6)
# ?svm
```
```{r, echo=FALSE}
plot(svm_fit, cut_calif)
# summary(svm_fit)
```

```{r}
svm_fit_y = predict(svm_fit, cut_calif[-1])
svm_fit_false_rate = sum(cut_calif$MedianOver!=svm_fit_y) / length(cut_calif$MedianOver)
print(1 - svm_fit_false_rate) #prediction accuracy # c=6, [1] 0.7584302
```

Still, classification performance of SVM based on accuracy is not satisfactory.
We may try to apply a non-linear kernel to the optimization problem, which is make SVM nice in the mathematical point of view, but I would skip it here.


### 6-b. Decision tree

To tuning a CART fit, only thing we should do is to select the cost function,
unless we want to give weight to each data point or scale some points.

I will use the Gini index for the classification tree here.
Another good option is the cross-entropy, but I will skip it.

```{r, warning=FALSE}
# CART
library(rpart)
# ?rpart
cart_fit = rpart(MedianOver~., data=cut_calif, method="class")
pruned_cart_fit = prune(cart_fit, cp=cart_fit$cptable[which.min(cart_fit$cptable[,"xerror"]),"CP"])
```

Note that, in the pruning step, any nodes were deleted in our case.

```{r, echo=FALSE}
# print(pruned_cart_fit, digit=2)
# pruned_cart_fit$cptable
plot(pruned_cart_fit, uniform=TRUE, main="Pruned Classification Tree")
text(pruned_cart_fit, use.n=TRUE, all=TRUE, cex=.8)

grid$pruned_cart_fit_on_grid = predict(pruned_cart_fit, grid, "class")
plot(grid$Longitude, grid$Latitude, col=grid$pruned_cart_fit_on_grid, pch=1)
points(Latitude ~ Longitude, data=cut_calif, col=cut_calif$MedianOver)
```

```{r}
cart_fit_y = predict(pruned_cart_fit, cut_calif[-1], "class")
cart_fit_false_rate = sum(cut_calif$MedianOver!=cart_fit_y)/length(cut_calif$MedianOver)
print(1 - cart_fit_false_rate)
```

The boundary seems of a union of retangular, which is natural for a tree.
Even if the boundary seems so crude, prediction accuracy is higher than the SVM classifier above.

### 6-c. Random forest

To tune a random-forest classifier, important tuning parameters may be the number of total trees, 
number of variable(covariate) randomly sampled as a candidates at each split, and vote rule.
Here, I just use a default values for 'randomForest' fuction in the 'randomForest' package.
The number of trees is 500, the number of candid covariates are $sqrt(p)$, and voting rule is just the majority voting.

```{r, warning=FALSE, message=FALSE}
#random forest
library(randomForest)
# ?randomForest
set.seed(20221017)
rf_fit = randomForest(MedianOver~., data=cut_calif, proximity=TRUE) #super time consuming, memory burst!!
```

```{r, echo=FALSE}
# print(rf_fit)

grid$rf_fit_on_grid = predict(rf_fit, grid)
plot(grid$Longitude, grid$Latitude, col=grid$rf_fit_on_grid, pch=1)
points(Latitude ~ Longitude, data=cut_calif, col=cut_calif$MedianOver)
```
```{r}
rf_fit_y = predict(rf_fit, cut_calif[-1])
rf_fit_false_rate = sum(cut_calif$MedianOver!=rf_fit_y)/length(cut_calif$MedianOver)
print(1 - rf_fit_false_rate)
```

The accuracy is preety high, but it seems too high so that we may concern about overfitting.
If we come back to the plot above, there are too narrow areas for each class, which makes the concern stronger.
More number of trees might be needed.

### Discussion

I heard that SVMs got an high-attention from the theoriests.
One reason is that it can be naturally extended to more general Hilbert spaces setting 
due to the optimal (fitted result) is only affected through inner product of data.
For example, lots of research seems to be conduct by chainging inner product of a space, 
sometimes combining RKHS, enjoying the space's good property to get a property of SVM fit.
Not only that, the motivation is based on 'hyperplane separation theorem' having a beauty from geometry, topology, and algebra.

Another good point is, SVMs deal with high demensional problem preety well, compared to other learning methods.
Curse of dimensionality exists, but seems weeker for SVM.

Not only that, SVMs give easily interpretable analytic expression of the classification boundary.

However, SVMs' performance is generally worse than other learning methods.
That's why so many pre-stage-SVM methods are developed.
Plus, the optimization is often very unstable, highly depending on choosing tuning parameter.
These things make practitioners annoyed to use SVM in practice.

Even if it seems that there isn't no research about uncertainty quantification (for example, getting standard error of a fit) about SVM,
in general, there are just few results about asymptotic behavior and SVM fit's uncertainty.
As a result, many people rely on bootstrap or other resampling methods, which requires SVM users to do additional works.

The CART, one of many decision tree classifiers, is very intuitive when considering its analytic form and tree expression.
It is just a way of fitting a value for each splited area.
In addition, in general, tree algorithms do not require pre-work like normalizing, scaling, or specific pre-processing.

Nonetheless, generally speaking, tree algorithm's performance is bad.
Prediction accuracy is not good compared to other learning algorithm.
That's why bagging and boosting are almost always needed when you want to use trees in practice.

Some other problems are, sometimes spliting algorithm gets to be super-complex, requiring too much computing power.
And generally tree algorithms are too senseitive with outliers. It is far from 'robust' algorithm.
And it is easy to be overfitted, so we take additional step like pruning.

Uncertainty quantification is ironically not bad, cause trees are almost always used with bagging or boosting.
Bagging gives a CV-based or bootstrap-based variance estimates as a by-product, and boosting yields asymptotically good behavior by taking mean.
This may be why some statisticians love trees.

The random forest, which is tree with bagging with an additional constraint on spliting, performs well in general.
By the reason that I said above, its fitting algorithm naturally gives its uncertainty estimates through its bagging step.
Not only that, the random forest is easy to extend and flexible to apply.
For example, we can use it for regression and classification. Plus, nowadays, the causal random forest has got huge attention from researchers.

However, running time is super long, (with my intel i5 chip, random forest fitting for the above dataset takes more than 10 minutes) making practitioners tired.
Generally, at least 500 bagging trees are recommended. If the number of trees are too low, the performance of random forest model gets super bad.
But nowadays, there is a huge competitor that we cannot ignore if long fitting time is available: deep neural network classifier.
If you do not need a uncertainty quantification, it is much better to use modern deep neural network(DNN) models to get higher prediction accuracy.
I am almost sure that if you take the same amount of time to fit both DNN and random forest, the DNN has better performance.

